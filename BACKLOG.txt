The focus is on the Top10 list
    - because this tracks the hottest movers

1. Trend Top10 symbols
    - Cumulative ranked list (of the day)
    - of Top 10 ticker symbols per minute
    - shows which symbols are up & coming & which are trailing off

2. Top10 Cold drop-offs
    - referencing #1 -> call-out symbols that drop-off the Top10
        - call-out...
        - 2.1 slow drop-offs
        - 2.2 Fast drop-offs

3. Top10 Hot new upstarts
    - referencing #1 -> call-out symbols that appear on Top10
    - all of a sudden, out of nowhere
    - based on the previous Top10 list over the past X mins (def=10mins)
        - Fast jumpers (symbol quickly appears in high ranked pos, not 10th )
        - Slow crawlers (symbol creeps in at 10th & moves up, 9th, 8th, 7th...)

4.  Top10 Losers

5.  Fast Migrations
    - Symbol moves Top10 Gainers -> Top10 Losers in same day
        - Sign of major problem with stock
    - Symbol moves from Top10 Losers -> Top10 Gainers in same day
        - Sign of hot stock

6.  Big swingers
    - Symbol has rapid big high/low swings in very short periods

7.  How to use Filters on finance.yahoo.com URL
    - for alt data extraction criteria, especially...
        - Market cap (smaller MCcap's have greater intraday volatility)
        - Intraday volume
        - sorted by % gain, then Cur_price with range filtering ($10-$50)

8.  Find the Slow Crawl Starter that ends up being a full day linear run-up
	- A: look at 1 min ticks over a 5 mins slice
		- (as early in the day as possible. Preferably starting @ 09:30)
	- B: look at 1 min - 5 min slice
		- Compute total 5 min gain
		- then look at cumulative gains between each detail slice
			- 1m - 2m = B1
			- 2m - 3m = B2
			- 3m - 4m = B3
			- 4m - 5m = B4
			- Nm - N+1m = BN
		- Ascertain
			- is B1 the greatest gain (i.e. a straight-up spike? across B1, B2, B3, B4)
			- are the remaining slice gains(B2, B3, B4) flat? i.e. >= B1 (i.e. small)
			or...
			- is B1 similar to B2, B3, B4.... (i.e. a steady % gain up, or linear, for <linear )
			or...
			look at a bigger slice, and figure out if A slow-start pattern happens somewhere else
			 in the slice window.

9.  Screener #1
        Personal screener logic
            1. Market Cap > 750M
            2. Avg closing price > $5.0
            3. % gain > 4%

10. News AI
	- Scan a bunch of key RSS Feeds for news on specific tickers that we carre interested in.
		- Exctract into usable data
		- Build a data set that can be input into SVM to classify the sentiment of news for a stock
			- Positive, Neutral, Negative via Naive Bayes classifier & shallow semantic parsing.
	- Possible RSS feeds to leverge...
		1. YAHOO.com news
			https://finance.yahoo.com/quote/<_SYMBOL>/news?p=<_SYMBOL>
			https://finance.yahoo.com/quote/<_SYMBOL>)/press-releases?p=<_SYMBOL_>
		2. GOOGLE news: https://news.google.com/rss/search?q=<your-text-here>
		3. NASDAQ RSS feeds - https://www.nasdaq.com/nasdaq-RSS-Feeds
			3. Market: https://www.nasdaq.com/feed/rssoutbound?category=Markets
			4. Stocks: https://www.nasdaq.com/feed/rssoutbound?category=Stocks
			5. An individual stock: https://www.nasdaq.com/feed/rssoutbound?symbol=lyft
			6. EARNINGS: https://www.nasdaq.com/feed/rssoutbound?category=Earnings
			7. MARKET specific: https://www.nasdaq.com/feed/rssoutbound?category=FinTech


11. Screener #2
	1. Unusual volume UP/DOWN
		Look at:
		https://old.nasdaq.com/markets/unusual-volume.aspx
		looks like it might be hackable

		This is an important dataset that is missing.
    - Initial template of this is done.

12. Individual stock logic
	1. first: need to find a data source

	2. How many days OPEN-DOWN & OPEN-UP
		- on these days...
			2.a was the end of day close, UP/FLAT/DOWN
			2.b what was the HIGHEST-HIGH for the day, and whats that as a % GAIN
			2.c what was the LOWEST-LOW for the day, and whats that as a % LOSS

	3. Identify the ahnomoly's in this pattern

13. Inference's across multiple DataFrames
    1. Is a stock appearing in the Top_10 + Screener + Unusual_vol
        - call it out

14. Craft a 'True Top % gainers' chart
	- This requires combining all dataframes from extracted market cap % ganers (small/med/large/mega)
	  into 1 true chart, since finaince.yahoo.com doesn't do this. They only have precanned charts that
	  seperate/isolate the data feeds from small_cap comapnies and medium/large/mega cap companies.
	- I already extract the data individually from each seperate html pre-canned chart page...so it wont be too
	  difficult to create a rue top gainers chart across all market cap ranges.
	- Logic thoughts:
		- merge all 3 Top 10 % giner lists A=(top 10 for medium/large/mega) + B=(top 10 for small) + C=(top 25 for unusual vol)
		- if a company appears multiple times across all dataframes...apply some logic to build a meaningful list
			- figure out which is it logical home A, B or C ?
			- include it only once in the final merged dataframe
			- the compensate for it being in multiple areas, pull another company from it's home
			  list (for fairness & completness).
			a. Logic #1
				- If home == small_cap
				- & it appears N times in total (in all lists)
				- Show it only once in the combo merged list
				- but pull in N-1 extra symbols from small_cap
			b. Logic #2
				- if it only appears in Unusual_vol but not in A or B
				- figurre out what its home is
				- add it to combo  merged lit
				- and pull in 1 extra symbol from it's home A or B
			c. Logic #3
				- if home =- large_cap
				- & it appears N times in total (i.e. in all lists)
				- hit is only once in the combo list
				- but pull in N-1 extra symbols from large_cap
			D. Logic #4
				- for any symbol that recies this special treatment
				- tag it in the final merged list as an outlier
				
15. Add a new CMDLine option
	- option is -f [flag symbol or an arbitrary list of symbols]
		- user enters a symbol ticker (e.g GOOG, INTC, SNAP etc)
		- and all dataframes will highlight (flag) that symbol in all outputs
	- as the number of dataframe tables increases, it's becomming more difficult to parse the output
	  with human eyes and find stuff in the tables.
				
